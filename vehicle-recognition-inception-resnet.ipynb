{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12144207,"sourceType":"datasetVersion","datasetId":7648568}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Training 2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Reshape, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom tqdm import tqdm\nimport albumentations as A # Import albumentations\n\n# --- 1. Configuration ---\nDATASET_PATH = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/\"\nTRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"train/images/\")\nTRAIN_LBL_DIR = os.path.join(DATASET_PATH, \"train/labels/\")\nVALID_IMG_DIR = os.path.join(DATASET_PATH, \"valid/images/\")\nVALID_LBL_DIR = os.path.join(DATASET_PATH, \"valid/labels/\")\n\nIMAGE_SIZE = (640, 640)\nNUM_CLASSES = 1\nBATCH_SIZE = 4\nEPOCHS = 30\nNUM_ANCHORS_TO_GENERATE = 9 # For K-Means\n\nLAMBDA_COORD = 5.0\nLAMBDA_NOOBJ = 0.5\nLAMBDA_OBJ = 1.0\nLAMBDA_CLASS = 1.0\n\n# --- 2. K-Means for Anchor Box Generation ---\ndef calculate_kmeans_anchors(label_dir, num_anchors, image_size):\n    \"\"\"\n    Reads all bounding box dimensions from the training labels and uses K-Means\n    clustering to find the optimal anchor boxes.\n    \"\"\"\n    print(f\"Calculating {num_anchors} anchors using K-Means...\")\n    box_dims = []\n    label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n\n    for label_file in tqdm(label_files, desc=\"Reading Bounding Boxes\"):\n        with open(os.path.join(label_dir, label_file), 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 5:\n                    # YOLO format: class x_center y_center width height\n                    # Convert normalized w, h to pixel values\n                    w = float(parts[3]) * image_size[1]\n                    h = float(parts[4]) * image_size[0]\n                    box_dims.append([w, h])\n\n    if not box_dims:\n        print(\"Warning: No bounding boxes found to calculate anchors. Using default.\")\n        return np.array([[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])\n\n    box_dims = np.array(box_dims)\n    kmeans = KMeans(n_clusters=num_anchors, random_state=42, n_init=10)\n    kmeans.fit(box_dims)\n    anchors = kmeans.cluster_centers_\n\n    # Sort anchors by area (width * height)\n    areas = anchors[:, 0] * anchors[:, 1]\n    sorted_indices = np.argsort(areas)\n    sorted_anchors = anchors[sorted_indices]\n\n    print(\"K-Means calculated anchors (width, height):\")\n    print(sorted_anchors.astype(int))\n    return sorted_anchors\n\n# Calculate anchors before initializing anything else that needs them\nANCHORS = calculate_kmeans_anchors(TRAIN_LBL_DIR, NUM_ANCHORS_TO_GENERATE, IMAGE_SIZE)\nANCHORS = np.array([\n    [10, 13], [16, 30], [33, 23],\n    [30, 61], [62, 45], [59, 119],\n    [116, 90], [156, 198], [373, 326]\n])\nNUM_ANCHORS = len(ANCHORS)\n\n# --- 3. Data Augmentation Pipeline ---\n# Define the augmentation pipeline using albumentations\n# This is much safer for object detection as it transforms bboxes correctly\ntransform = A.Compose([\n    A.RandomBrightnessContrast(p=0.2),\n    A.HueSaturationValue(p=0.2),\n    A.GaussNoise(p=0.2),\n    A.Blur(blur_limit=3, p=0.1),\n    # Geometric transformations that affect bounding boxes\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5,\n                       border_mode=cv2.BORDER_CONSTANT, value=0),\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'],\n                              min_visibility=0.1, min_area=1))\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport cv2\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, image_dir, label_dir, batch_size, image_size, anchors, num_classes, grid_size, augment=False, shuffle=True):\n        self.image_dir = image_dir\n        self.label_dir = label_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.grid_size = grid_size\n        self.augment = augment\n        self.shuffle = shuffle\n        self.image_filenames = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.indexes = np.arange(len(self.image_filenames))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        return int(np.floor(len(self.image_filenames) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_filenames = [self.image_filenames[i] for i in batch_indexes]\n        X, y = self.__data_generation(batch_filenames)\n        return X, y\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, batch_filenames):\n        X = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32)\n        y = np.zeros((self.batch_size, *self.grid_size, self.num_anchors, 5 + self.num_classes), dtype=np.float32)\n\n        for i, filename in enumerate(batch_filenames):\n            image_path = os.path.join(self.image_dir, filename + '.jpg')\n            image = cv2.imread(image_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            bboxes = []\n            class_labels = []\n            label_path = os.path.join(self.label_dir, filename + '.txt')\n            if os.path.exists(label_path):\n                with open(label_path, 'r') as f:\n                    for line in f:\n                        parts = line.strip().split()\n                        if len(parts) == 5:\n                            class_id = int(parts[0])\n                            bbox = list(map(float, parts[1:]))\n                            bboxes.append(bbox)\n                            class_labels.append(class_id)\n\n            # Apply augmentation (if any)\n            \n            if self.augment:\n                transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n                image = transformed['image']\n                bboxes = transformed['bboxes']\n                class_labels = transformed['class_labels']\n            \n            \n\n            image_resized = cv2.resize(image, self.image_size)\n            X[i] = image_resized / 255.0\n\n            for j, bbox in enumerate(bboxes):\n                class_id = int(class_labels[j])\n\n                # --- FIX 1: Validate class ID ---\n                if class_id < self.num_classes:\n                    x_center, y_center, w, h = bbox\n\n                    # --- FIX 2: Clamp grid coordinates ---\n                    grid_x = min(int(x_center * self.grid_size[1]), self.grid_size[1] - 1)\n                    grid_y = min(int(y_center * self.grid_size[0]), self.grid_size[0] - 1)\n\n                    gt_box = np.array([0, 0, w, h])\n                    best_iou = -1\n                    best_anchor_idx = -1\n                    for anchor_idx, anchor in enumerate(self.anchors):\n                        anchor_box = np.array([0, 0, anchor[0] / self.image_size[1], anchor[1] / self.image_size[0]])\n                        iou = self.iou(gt_box, anchor_box)\n                        if iou > best_iou:\n                            best_iou = iou\n                            best_anchor_idx = anchor_idx\n\n                    if best_anchor_idx != -1:\n                        y[i, grid_y, grid_x, best_anchor_idx, 0:4] = [x_center, y_center, w, h]\n                        y[i, grid_y, grid_x, best_anchor_idx, 4] = 1\n                        y[i, grid_y, grid_x, best_anchor_idx, 5 + class_id] = 1  # Safe now\n\n        return X, y\n\n    @staticmethod\n    def iou(box1, box2):\n        w1, h1 = box1[2], box1[3]\n        w2, h2 = box2[2], box2[3]\n        inter_w = min(w1, w2)\n        inter_h = min(h1, h2)\n        inter_area = inter_w * inter_h\n        union_area = w1 * h1 + w2 * h2 - inter_area\n        return inter_area / (union_area + 1e-6)\n\n\n# --- 4. Model Definition with Modified Grid Size ---\ndef build_model_640(target_grid_size=(18, 18)):\n    base_model = tf.keras.applications.InceptionResNetV2(\n        input_shape=(*IMAGE_SIZE, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n    \n    input_tensor = base_model.input\n    # This layer in InceptionResNetV2 produces an 18x18 feature map for a 640x640 input\n    backbone_output = base_model.get_layer('block8_6_ac').output\n    \n    print(f\"Original backbone output shape: {backbone_output.shape}\")\n\n    # --- MODIFICATION TO GET 20x20 GRID ---\n    # We use a Lambda layer with tf.image.resize to force the feature map to the desired size.\n    # This is a flexible way to handle mismatches between backbone output and desired grid size.\n    x = backbone_output\n    print(f\"Resized feature map shape: {x.shape}\")\n\n    x = Conv2D(512, (1, 1), padding='same')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    x = Conv2D(1024, (3, 3), padding='same')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    x = Conv2D(512, (1, 1), padding='same')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    \n    num_filters = NUM_ANCHORS * (5 + NUM_CLASSES)\n    predictions_raw = Conv2D(num_filters, (1, 1), padding='same', name='detection_head')(x)\n    \n    model = Model(inputs=input_tensor, outputs=predictions_raw)\n    return model\n\n# Define the ideal grid size\nIDEAL_GRID_SIZE = (18,18)\n\n# Build the model\nmodel = build_model_640(target_grid_size=IDEAL_GRID_SIZE)\nmodel.summary()\n\n# The grid size is now determined by our target, not the model's natural output\nMODEL_GRID_H, MODEL_GRID_W = IDEAL_GRID_SIZE\n\n# --- 5. Training Setup ---\n# The yolo_loss function depends on the global ANCHORS variable.\n# We need to make sure it's accessible or redefine it to accept anchors as an argument.\n# For simplicity, we'll keep it as a global dependency.\ndef yolo_loss(y_true, y_pred):\n    # This loss function remains the same as your original code\n    y_pred = tf.reshape(y_pred, tf.shape(y_true))\n    true_box_xy = y_true[..., 0:2]\n    true_box_wh = y_true[..., 2:4]\n    object_mask = y_true[..., 4:5]\n    true_class_probs = y_true[..., 5:]\n    \n    # Grid and anchor scaling\n    grid_shape = tf.shape(y_true)[1:3]\n    grid_h, grid_w = grid_shape[0], grid_shape[1]\n    \n    anchors_tensor = tf.constant(ANCHORS, dtype=tf.float32)\n    \n    pred_box_xy = tf.sigmoid(y_pred[..., 0:2])\n    pred_box_wh_offset = y_pred[..., 2:4]\n    pred_objectness = tf.sigmoid(y_pred[..., 4:5])\n    pred_class_probs = tf.nn.softmax(y_pred[..., 5:])\n    \n    # Decode predicted WH\n    pred_box_wh_pixels = tf.exp(pred_box_wh_offset) * anchors_tensor\n    \n    # Scale true wh to image size for loss calculation\n    true_box_wh_pixels = true_box_wh * tf.constant([IMAGE_SIZE[1], IMAGE_SIZE[0]], dtype=tf.float32)\n\n    # Loss calculation\n    box_loss_scale = 2.0 - true_box_wh[..., 0] * true_box_wh[..., 1]\n    \n    xy_loss = 3*LAMBDA_COORD * tf.reduce_sum(\n        object_mask * box_loss_scale[..., tf.newaxis] * tf.square(true_box_xy - pred_box_xy),\n        axis=[1, 2, 3, 4]\n    )\n    wh_loss = LAMBDA_COORD * tf.reduce_sum(\n        object_mask * box_loss_scale[..., tf.newaxis] * tf.square(tf.sqrt(true_box_wh_pixels + 1e-6) - tf.sqrt(pred_box_wh_pixels + 1e-6)),\n        axis=[1, 2, 3, 4]\n    )\n    coord_loss = xy_loss + wh_loss\n    \n    objectness_bce = tf.keras.losses.binary_crossentropy(object_mask, pred_objectness)\n    object_mask_squeezed = tf.squeeze(object_mask, axis=-1)\n    \n    obj_loss = LAMBDA_OBJ * tf.reduce_sum(objectness_bce * object_mask_squeezed, axis=[1, 2, 3])\n    noobj_loss = LAMBDA_NOOBJ * tf.reduce_sum(objectness_bce * (1 - object_mask_squeezed), axis=[1, 2, 3])\n    objectness_loss = obj_loss + noobj_loss\n    \n    class_bce = tf.keras.losses.binary_crossentropy(true_class_probs, pred_class_probs)\n    class_loss = LAMBDA_CLASS * tf.reduce_sum(class_bce * object_mask_squeezed, axis=[1, 2, 3])\n    \n    total_loss = coord_loss + objectness_loss + class_loss\n    return tf.reduce_mean(total_loss)\n\n# Instantiate generators with the new `augment` flag for training\ntrain_generator = DataGenerator(\n    TRAIN_IMG_DIR, TRAIN_LBL_DIR, BATCH_SIZE, IMAGE_SIZE, ANCHORS,\n    NUM_CLASSES, (MODEL_GRID_H, MODEL_GRID_W), augment=True, shuffle=True\n)\nvalidation_generator = DataGenerator(\n    VALID_IMG_DIR, VALID_LBL_DIR, BATCH_SIZE, IMAGE_SIZE, ANCHORS,\n    NUM_CLASSES, (MODEL_GRID_H, MODEL_GRID_W), augment=False, shuffle=False\n)\n\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=yolo_loss)\n\n# Callbacks remain the same\ncheckpoint = ModelCheckpoint(\n    'best_model_38x38.weights.h5', monitor='val_loss', save_best_only=True,\n    save_weights_only=True, mode='min', verbose=1\n)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n\nprint(\"\\n--- Starting Training ---\")\nhistory = model.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    callbacks=[checkpoint, reduce_lr, early_stop]\n)\n\n# The rest of your code for fine-tuning, plotting, and inference can remain the same.\n# Just make sure to load the new weights file 'best_model_enhanced.weights.h5'.\n# The decode_and_nms function will work correctly as it dynamically determines the grid size from the prediction tensor.\n\n# --- Example of running the rest of the script ---\nprint(\"\\n--- STAGE 2: Unfreezing backbone and fine-tuning ---\")\nmodel.trainable = True\nmodel.compile(\n    optimizer=Adam(learning_rate=5e-5),\n    loss=yolo_loss\n)\nhistory_fine_tune = model.fit(\n    train_generator,\n    epochs=EPOCHS * 2, # Maybe train for more epochs in fine-tuning\n    validation_data=validation_generator,\n    initial_epoch=history.epoch[-1] + 1,\n    callbacks=[checkpoint, reduce_lr, early_stop])\n\nplt.figure(figsize=(12, 4))\nplt.plot(history.history['loss'] + history_fine_tune.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'] + history_fine_tune.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss Over Epochs (Transfer + Fine-tuning)')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#16th june 2 pm ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Input, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom tqdm import tqdm\nimport albumentations as A\n\n# --- 1. Configuration ---\nDATASET_PATH = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/\"\nTRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"train/images/\")\nTRAIN_LBL_DIR = os.path.join(DATASET_PATH, \"train/labels/\")\nVALID_IMG_DIR = os.path.join(DATASET_PATH, \"valid/images/\")\nVALID_LBL_DIR = os.path.join(DATASET_PATH, \"valid/labels/\")\n\nIMAGE_SIZE = (640, 640)\nBOX_PARAMS = 5 # x, y, w, h, confidence\nBATCH_SIZE = 4\n# --- CHANGE: Increased training epochs ---\nEPOCHS = 50\nNUM_ANCHORS_TO_GENERATE = 9\n\n# --- CHANGE: Increased weight for object confidence loss ---\nLAMBDA_COORD = 5.0\nLAMBDA_NOOBJ = 0.5\nLAMBDA_OBJ = 2.5 # Was 1.0. This will push the model to be more confident.\n\n# --- 2. K-Means for Anchor Box Generation (No changes) ---\ndef calculate_kmeans_anchors(label_dir, num_anchors, image_size):\n    print(f\"Calculating {num_anchors} anchors using K-Means...\")\n    box_dims = []\n    label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n    for label_file in tqdm(label_files, desc=\"Reading Bounding Boxes\"):\n        with open(os.path.join(label_dir, label_file), 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 5:\n                    w = float(parts[3]) * image_size[1]\n                    h = float(parts[4]) * image_size[0]\n                    box_dims.append([w, h])\n    if not box_dims:\n        print(\"Warning: No bounding boxes found. Using default.\")\n        return np.array([[10, 13], [16, 30], [33, 23], [30, 61], [62, 45], [59, 119], [116, 90], [156, 198], [373, 326]])\n    box_dims = np.array(box_dims)\n    kmeans = KMeans(n_clusters=num_anchors, random_state=42, n_init=10)\n    kmeans.fit(box_dims)\n    anchors = kmeans.cluster_centers_\n    areas = anchors[:, 0] * anchors[:, 1]\n    sorted_indices = np.argsort(areas)\n    sorted_anchors = anchors[sorted_indices]\n    print(\"K-Means calculated anchors (width, height):\")\n    print(sorted_anchors.astype(int))\n    return sorted_anchors\n\nANCHORS = calculate_kmeans_anchors(TRAIN_LBL_DIR, NUM_ANCHORS_TO_GENERATE, IMAGE_SIZE)\nNUM_ANCHORS = len(ANCHORS)\n\n# --- 3. Data Augmentation Pipeline (No changes) ---\ntransform = A.Compose([\n    A.RandomBrightnessContrast(p=0.2),\n    A.HueSaturationValue(p=0.2),\n    A.GaussNoise(p=0.2),\n    A.Blur(blur_limit=3, p=0.1),\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5,\n                       border_mode=cv2.BORDER_CONSTANT, value=0),\n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'],\n                              min_visibility=0.1, min_area=1))\n\n# --- 4. DataGenerator (No changes) ---\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, image_dir, label_dir, batch_size, image_size, anchors, grid_size, augment=False, shuffle=True):\n        self.image_dir = image_dir\n        self.label_dir = label_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.grid_size = grid_size\n        self.augment = augment\n        self.shuffle = shuffle\n        self.image_filenames = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.indexes = np.arange(len(self.image_filenames))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        return int(np.floor(len(self.image_filenames) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        batch_filenames = [self.image_filenames[i] for i in batch_indexes]\n        X, y = self.__data_generation(batch_filenames)\n        return X, y\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, batch_filenames):\n        X = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32)\n        y = np.zeros((self.batch_size, *self.grid_size, self.num_anchors, BOX_PARAMS), dtype=np.float32)\n        for i, filename in enumerate(batch_filenames):\n            image_path = os.path.join(self.image_dir, filename + '.jpg')\n            image = cv2.imread(image_path)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            bboxes, class_labels = [], []\n            label_path = os.path.join(self.label_dir, filename + '.txt')\n            if os.path.exists(label_path):\n                with open(label_path, 'r') as f:\n                    for line in f:\n                        parts = line.strip().split()\n                        if len(parts) == 5:\n                            bboxes.append(list(map(float, parts[1:])))\n                            class_labels.append(int(parts[0]))\n            if self.augment:\n                transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n                image, bboxes = transformed['image'], transformed['bboxes']\n            image_resized = cv2.resize(image, self.image_size)\n            X[i] = image_resized / 255.0\n            for bbox in bboxes:\n                x_center, y_center, w, h = bbox\n                grid_x = min(int(x_center * self.grid_size[1]), self.grid_size[1] - 1)\n                grid_y = min(int(y_center * self.grid_size[0]), self.grid_size[0] - 1)\n                gt_box = np.array([0, 0, w, h])\n                best_iou, best_anchor_idx = -1, -1\n                for anchor_idx, anchor in enumerate(self.anchors):\n                    anchor_box = np.array([0, 0, anchor[0] / self.image_size[1], anchor[1] / self.image_size[0]])\n                    iou = self.iou(gt_box, anchor_box)\n                    if iou > best_iou:\n                        best_iou, best_anchor_idx = iou, anchor_idx\n                if best_anchor_idx != -1:\n                    y[i, grid_y, grid_x, best_anchor_idx, 0:4] = [x_center, y_center, w, h]\n                    y[i, grid_y, grid_x, best_anchor_idx, 4] = 1\n        return X, y\n\n    @staticmethod\n    def iou(box1, box2):\n        w1, h1, w2, h2 = box1[2], box1[3], box2[2], box2[3]\n        inter_area = min(w1, w2) * min(h1, h2)\n        union_area = w1 * h1 + w2 * h2 - inter_area\n        return inter_area / (union_area + 1e-6)\n\n# --- 5. Model Definition ---\ndef build_model_38x38():\n    base_model = tf.keras.applications.InceptionResNetV2(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n    base_model.trainable = False\n    input_tensor = base_model.input\n    # --- FIX: Correcting the print statement to match the layer being used ---\n    backbone_output = base_model.get_layer('block17_20_ac').output \n    print(f\"Using backbone output from 'block17_20_ac' with shape: {backbone_output.shape}\")\n    x = Conv2D(512, (1, 1), padding='same', activation=LeakyReLU(alpha=0.1))(backbone_output)\n    x = Conv2D(1024, (3, 3), padding='same', activation=LeakyReLU(alpha=0.1))(x)\n    num_filters = NUM_ANCHORS * BOX_PARAMS\n    predictions_raw = Conv2D(num_filters, (1, 1), padding='same', name='detection_head')(x)\n    model = Model(inputs=input_tensor, outputs=predictions_raw)\n    return model\n\n# --- 6. Loss Function (No changes) ---\ndef yolo_loss_simplified(y_true, y_pred):\n    y_pred = tf.reshape(y_pred, tf.shape(y_true))\n    true_box_xy = y_true[..., 0:2]\n    true_box_wh = y_true[..., 2:4]\n    object_mask = y_true[..., 4:5]\n    pred_box_xy = tf.sigmoid(y_pred[..., 0:2])\n    pred_box_wh_offset = y_pred[..., 2:4]\n    pred_objectness = tf.sigmoid(y_pred[..., 4:5])\n    anchors_tensor = tf.constant(ANCHORS, dtype=tf.float32)\n    pred_box_wh_pixels = tf.exp(pred_box_wh_offset) * anchors_tensor\n    true_box_wh_pixels = true_box_wh * tf.constant([IMAGE_SIZE[1], IMAGE_SIZE[0]], dtype=tf.float32)\n    box_loss_scale = 2.0 - true_box_wh[..., 0] * true_box_wh[..., 1]\n    xy_loss = LAMBDA_COORD * tf.reduce_sum(object_mask * box_loss_scale[..., tf.newaxis] * tf.square(true_box_xy - pred_box_xy), axis=[1, 2, 3, 4])\n    wh_loss = LAMBDA_COORD * tf.reduce_sum(object_mask * box_loss_scale[..., tf.newaxis] * tf.square(tf.sqrt(true_box_wh_pixels + 1e-6) - tf.sqrt(pred_box_wh_pixels + 1e-6)), axis=[1, 2, 3, 4])\n    coord_loss = xy_loss + wh_loss\n    objectness_bce = tf.keras.losses.binary_crossentropy(object_mask, pred_objectness)\n    object_mask_squeezed = tf.squeeze(object_mask, axis=-1)\n    obj_loss = LAMBDA_OBJ * tf.reduce_sum(objectness_bce * object_mask_squeezed, axis=[1, 2, 3])\n    noobj_loss = LAMBDA_NOOBJ * tf.reduce_sum(objectness_bce * (1 - object_mask_squeezed), axis=[1, 2, 3])\n    objectness_loss = obj_loss + noobj_loss\n    total_loss = coord_loss + objectness_loss\n    return tf.reduce_mean(total_loss)\n\n# --- 7. Build and Train ---\nmodel = build_model_38x38()\nmodel.summary()\n\nMODEL_GRID_H, MODEL_GRID_W = model.output_shape[1:3]\nprint(f\"Model grid size set to: ({MODEL_GRID_H}, {MODEL_GRID_W})\")\n\ntrain_generator = DataGenerator(TRAIN_IMG_DIR, TRAIN_LBL_DIR, BATCH_SIZE, IMAGE_SIZE, ANCHORS, (MODEL_GRID_H, MODEL_GRID_W), augment=True, shuffle=True)\nvalidation_generator = DataGenerator(VALID_IMG_DIR, VALID_LBL_DIR, BATCH_SIZE, IMAGE_SIZE, ANCHORS, (MODEL_GRID_H, MODEL_GRID_W), augment=False, shuffle=False)\n\n# Compile for Stage 1\nmodel.compile(optimizer=Adam(learning_rate=1e-4), loss=yolo_loss_simplified)\n\n# Callbacks\ncheckpoint = ModelCheckpoint('best_model_38x38_retrain.weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1) # Increased patience\nearly_stop = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1) # Increased patience\n\nprint(\"\\n--- Starting Training (Stage 1) ---\")\nhistory = model.fit(\n    train_generator,\n    epochs=EPOCHS, # Now 50\n    validation_data=validation_generator,\n    callbacks=[checkpoint, reduce_lr, early_stop]\n)\n\n# --- CHANGE: Fine-tuning stage is re-enabled and extended ---\nprint(\"\\n--- STAGE 2: Unfreezing backbone and fine-tuning ---\")\nmodel.trainable = True\n\n# It's good practice to re-compile after changing trainable status\nmodel.compile(\n    optimizer=Adam(learning_rate=1e-5), # Use a low learning rate for fine-tuning\n    loss=yolo_loss_simplified\n)\nprint(\"Model re-compiled for fine-tuning with a lower learning rate.\")\n\nhistory_fine_tune = model.fit(\n    train_generator,\n    epochs=EPOCHS * 2, # Fine-tune for 100 epochs\n    validation_data=validation_generator,\n    initial_epoch=history.epoch[-1] + 1 if history.epoch else 0,\n    callbacks=[checkpoint, reduce_lr, early_stop]\n)\n\n# --- CHANGE: Robust plotting code ---\nprint(\"\\n--- Generating Loss Plot ---\")\ncombined_loss = history.history.get('loss', [])\ncombined_val_loss = history.history.get('val_loss', [])\n\n# Check if fine-tuning actually ran and has a history\nif hasattr(history_fine_tune, 'history') and history_fine_tune.history:\n    print(\"Combining fine-tuning history for plotting.\")\n    combined_loss.extend(history_fine_tune.history.get('loss', []))\n    combined_val_loss.extend(history_fine_tune.history.get('val_loss', []))\nelse:\n    print(\"No fine-tuning history found to plot.\")\n\n# Plot the combined histories\nplt.figure(figsize=(12, 5))\nif combined_loss:\n    plt.plot(combined_loss, label='Training Loss')\nif combined_val_loss:\n    plt.plot(combined_val_loss, label='Validation Loss')\n\nplt.title('Model Loss Over Epochs (38x38 Grid - Retrained)')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Add a vertical line to show where fine-tuning started, if it happened\nif hasattr(history_fine_tune, 'epoch') and history_fine_tune.epoch:\n    start_fine_tune_epoch = history.epoch[-1] + 1 if history.epoch else 0\n    plt.axvline(x=start_fine_tune_epoch, color='r', linestyle='--', label='Fine-tuning starts')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# --- PRE-REQUISITES (Ensure these match your training script) ---\n# --- Configuration ---\nVALID_IMG_DIR = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images/\"\nVALID_LBL_DIR = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/labels/\"\nIMAGE_SIZE = (640, 640)\nBOX_PARAMS = 5  # x, y, w, h, confidence\nANCHORS = np.array([\n    # This should be the same ANCHORS array used during training\n    [20, 29], [33,45], [48, 64],\n    [63,91], [78, 140], [109,105],\n    [151,201], [208,356], [396,618]\n])\n\nNUM_ANCHORS = len(ANCHORS)\n\n\n# --- Model Loading ---\n# Re-define the model building function exactly as it was during training\ndef build_model_38x38():\n    base_model = tf.keras.applications.InceptionResNetV2(\n        input_shape=(*IMAGE_SIZE, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n    input_tensor = base_model.input\n    backbone_output = base_model.get_layer('block17_20_ac').output\n    x = tf.keras.layers.Conv2D(512, (1, 1), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1))(backbone_output)\n    x = tf.keras.layers.Conv2D(1024, (3, 3), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.1))(x)\n    num_filters = NUM_ANCHORS * BOX_PARAMS\n    predictions_raw = tf.keras.layers.Conv2D(num_filters, (1, 1), padding='same', name='detection_head')(x)\n    model = tf.keras.models.Model(inputs=input_tensor, outputs=predictions_raw)\n    return model\n\n# 1. Build the model architecture\nprint(\"Building model architecture...\")\nmodel = build_model_38x38()\n\n# 2. Load the trained weights\nprint(\"Loading trained weights from 'best_model_38x38.weights.h5'...\")\nmodel.load_weights('best_model_38x38.weights.h5')\nprint(\"Weights loaded successfully.\")\n\n\n# --- Helper Functions for Evaluation ---\n# --- MODIFIED: Simplified decode_predictions for the new model output ---\ndef decode_predictions_simplified(raw_preds):\n    \"\"\"\n    Decodes the raw output of the simplified YOLO model (no class prediction).\n    \"\"\"\n    grid_h, grid_w = raw_preds.shape[1:3]\n    \n    # Reshape to (batch, grid_h, grid_w, num_anchors, 5)\n    predictions = tf.reshape(raw_preds, (tf.shape(raw_preds)[0], grid_h, grid_w, NUM_ANCHORS, BOX_PARAMS))\n\n    # Get individual components\n    pred_xy = tf.sigmoid(predictions[..., 0:2])\n    pred_wh_offset = predictions[..., 2:4]\n    pred_conf = tf.sigmoid(predictions[..., 4:5])\n\n    # Create a grid for calculating absolute coordinates\n    grid = tf.meshgrid(tf.range(grid_w, dtype=tf.float32), tf.range(grid_h, dtype=tf.float32))\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n\n    # Decode box coordinates\n    box_xy_grid = pred_xy + grid\n    box_xy_normalized = box_xy_grid / tf.constant([grid_w, grid_h], dtype=tf.float32)\n    \n    anchors_tensor = tf.constant(ANCHORS, dtype=tf.float32)\n    box_wh_pixels = tf.exp(pred_wh_offset) * anchors_tensor\n    box_wh_normalized = box_wh_pixels / tf.constant([IMAGE_SIZE[1], IMAGE_SIZE[0]], dtype=tf.float32)\n\n    # Convert from (center_x, center_y, w, h) to (x1, y1, x2, y2)\n    box_x1y1 = box_xy_normalized - (box_wh_normalized / 2.0)\n    box_x2y2 = box_xy_normalized + (box_wh_normalized / 2.0)\n    boxes = tf.concat([box_x1y1, box_x2y2], axis=-1)\n\n    # Flatten the grid/anchor dimensions to get a list of predictions per image\n    boxes_flat = tf.reshape(boxes, (tf.shape(boxes)[0], -1, 4))\n    scores_flat = tf.reshape(pred_conf, (tf.shape(pred_conf)[0], -1))\n    \n    # Since there's only one class, the class ID is always 0\n    classes_flat = tf.zeros_like(scores_flat, dtype=tf.int32)\n    \n    return boxes_flat, scores_flat, classes_flat\n\n\ndef calculate_iou(box1, box2):\n    # This function remains the same\n    x1_inter = np.maximum(box1[:, 0], box2[:, 0])\n    y1_inter = np.maximum(box1[:, 1], box2[:, 1])\n    x2_inter = np.minimum(box1[:, 2], box2[:, 2])\n    y2_inter = np.minimum(box1[:, 3], box2[:, 3])\n    inter_area = np.maximum(0.0, x2_inter - x1_inter) * np.maximum(0.0, y2_inter - y1_inter)\n    box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n    box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n    union_area = box1_area + box2_area - inter_area\n    return inter_area / (union_area + 1e-6)\n\n# --- Main Evaluation Logic ---\ndef evaluate_model(model, img_dir, lbl_dir, conf_thresh=0.1, nms_thresh=0.5, iou_threshold=0.5):\n    all_detections, all_ground_truths = [], []\n    validation_files = [os.path.splitext(f)[0] for f in os.listdir(img_dir) if f.endswith('.jpg')]\n\n    print(f\"\\nRunning evaluation on {len(validation_files)} validation images...\")\n    \n    for i, filename in enumerate(tqdm(validation_files, desc=\"Evaluating\")):\n        # Get Predictions\n        image_path = os.path.join(img_dir, filename + '.jpg')\n        image = cv2.imread(image_path)\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n        input_data = np.expand_dims(image_resized / 255.0, axis=0)\n        raw_preds = model.predict(input_data, verbose=0)\n        \n        # Use the new simplified decoder\n        pred_boxes, pred_scores, pred_classes = decode_predictions_simplified(raw_preds)\n        \n        selected_indices = tf.image.non_max_suppression(\n            boxes=pred_boxes[0],\n            scores=pred_scores[0],\n            max_output_size=50,\n            iou_threshold=nms_thresh,\n            score_threshold=conf_thresh\n        )\n        \n        final_boxes = tf.gather(pred_boxes[0], selected_indices).numpy()\n        final_scores = tf.gather(pred_scores[0], selected_indices).numpy()\n        final_classes = tf.gather(pred_classes[0], selected_indices).numpy()\n        \n        for box, score, cls in zip(final_boxes, final_scores, final_classes):\n            all_detections.append([i, int(cls), float(score), *box])\n            \n        # Get Ground Truths\n        label_path = os.path.join(lbl_dir, filename + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    class_id = int(parts[0]) # This will always be 0\n                    x_c, y_c, w, h = map(float, parts[1:])\n                    x1, y1 = x_c - w / 2, y_c - h / 2\n                    x2, y2 = x_c + w / 2, y_c + h / 2\n                    all_ground_truths.append([i, class_id, 0, x1, y1, x2, y2])\n                    \n    # Calculate mAP\n    # Since we only have one class, mAP is just the AP of that class.\n    detections_c = [d for d in all_detections if d[1] == 0]\n    ground_truths_c = [gt for gt in all_ground_truths if gt[1] == 0]\n\n    ap = 0.0\n    if len(ground_truths_c) > 0:\n        detections_c.sort(key=lambda x: x[2], reverse=True)\n        num_gt_boxes = len(ground_truths_c)\n        true_positives = np.zeros(len(detections_c))\n        false_positives = np.zeros(len(detections_c))\n        gt_matched = {}\n\n        for det_idx, detection in enumerate(detections_c):\n            img_idx = detection[0]\n            gts_in_image = [gt for gt in ground_truths_c if gt[0] == img_idx]\n            if len(gts_in_image) == 0:\n                false_positives[det_idx] = 1\n                continue\n            det_box = np.array([detection[3:]])\n            gt_boxes = np.array([gt[3:] for gt in gts_in_image])\n            ious = calculate_iou(det_box, gt_boxes)\n            best_iou_idx = np.argmax(ious)\n            if ious[best_iou_idx] >= iou_threshold:\n                if gt_matched.get(img_idx) is None: gt_matched[img_idx] = []\n                if best_iou_idx not in gt_matched[img_idx]:\n                    true_positives[det_idx] = 1\n                    gt_matched[img_idx].append(best_iou_idx)\n                else:\n                    false_positives[det_idx] = 1\n            else:\n                false_positives[det_idx] = 1\n        tp_cumsum = np.cumsum(true_positives)\n        fp_cumsum = np.cumsum(false_positives)\n        recalls = tp_cumsum / (num_gt_boxes + 1e-6)\n        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n        precisions = np.concatenate(([1.0], precisions))\n        recalls = np.concatenate(([0.0], recalls))\n        for i in range(len(precisions) - 2, -1, -1):\n            precisions[i] = np.maximum(precisions[i], precisions[i+1])\n        recall_diff = recalls[1:] - recalls[:-1]\n        ap = np.sum(recall_diff * precisions[1:])\n    \n    print(\"\\n--- Evaluation Metrics ---\")\n    print(f\"Confidence Threshold: {conf_thresh}\")\n    print(f\"NMS Threshold: {nms_thresh}\")\n    print(f\"IoU Threshold for mAP: {iou_threshold}\")\n    print(\"--------------------------\")\n    print(f\"Average Precision (AP) for Vehicle: {ap:.4f}\")\n    \n    return ap\n\ndef visualize_evaluation(model, img_dir, lbl_dir, num_images=5, conf_thresh=0.5, nms_thresh=0.1):\n    # This visualization function is adapted for the new model\n    validation_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n    if len(validation_files) < num_images: num_images = len(validation_files)\n    sample_files = np.random.choice(validation_files, num_images, replace=False)\n\n    for filename in sample_files:\n        image_path = os.path.join(img_dir, filename)\n        image = cv2.imread(image_path)\n        original_h, original_w, _ = image.shape\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n        input_data = np.expand_dims(image_resized / 255.0, axis=0)\n        raw_preds = model.predict(input_data, verbose=0)\n        \n        pred_boxes, pred_scores, _ = decode_predictions_simplified(raw_preds)\n        selected_indices = tf.image.non_max_suppression(\n            boxes=pred_boxes[0], scores=pred_scores[0], max_output_size=50,\n            iou_threshold=nms_thresh, score_threshold=conf_thresh\n        )\n        final_boxes = tf.gather(pred_boxes[0], selected_indices).numpy()\n        final_scores = tf.gather(pred_scores[0], selected_indices).numpy()\n\n        for box, score in zip(final_boxes, final_scores):\n            x1, y1, x2, y2 = box\n            x1, y1 = int(x1 * original_w), int(y1 * original_h)\n            x2, y2 = int(x2 * original_w), int(y2 * original_h)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            label = f\"Vehicle: {score:.2f}\"\n            cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Draw Ground Truth Boxes\n        label_path = os.path.join(lbl_dir, os.path.splitext(filename)[0] + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    _, x_c, y_c, w, h = map(float, line.strip().split())\n                    x1, y1 = int((x_c - w/2) * original_w), int((y_c - h/2) * original_h)\n                    x2, y2 = int((x_c + w/2) * original_w), int((y_c + h/2) * original_h)\n                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n        \n        plt.figure(figsize=(10, 10))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Evaluation: {filename}\")\n        plt.axis('off')\n        plt.show()\n\n# --- Run the Evaluation ---\nif __name__ == '__main__':\n    # Calculate AP@0.5\n    evaluate_model(model, VALID_IMG_DIR, VALID_LBL_DIR, iou_threshold=0.2, conf_thresh=0.1)\n    \n    # Visualize some results with a slightly higher confidence for cleaner plots\n    print(\"\\nDisplaying sample evaluation images (Green=Prediction, Blue=Ground Truth)...\")\n    visualize_evaluation(model, VALID_IMG_DIR, VALID_LBL_DIR, num_images=5, conf_thresh=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# --- PRE-REQUISITES (Ensure these are defined from your training script) ---\n# Make sure these variables match your training setup EXACTLY.\n\n# --- Configuration ---\nVALID_IMG_DIR = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images/\"\nVALID_LBL_DIR = \"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/labels/\"\nIMAGE_SIZE = (640, 640)\nNUM_CLASSES = 1\nNUM_ANCHORS = 9\nconfidence_thresh = 0.5\nnms_thresh = 0.5\niou_threshold = 0.5\n\n# --- Model Loading ---\n# You need the model building function to create the architecture first\n# Make sure this function is identical to the one you used for training.\ndef build_model_640():\n    \n        base_model = tf.keras.applications.InceptionResNetV2(\n            input_shape=(*IMAGE_SIZE, 3),\n            include_top=False,\n            weights='imagenet'\n        )\n        base_model.trainable = False\n        \n        input_tensor = base_model.input\n        # This layer in InceptionResNetV2 produces an 18x18 feature map for a 640x640 input\n        backbone_output = base_model.get_layer('block8_6_ac').output\n        \n        print(f\"Original backbone output shape: {backbone_output.shape}\")\n    \n        # --- MODIFICATION TO GET 20x20 GRID ---\n        # We use a Lambda layer with tf.image.resize to force the feature map to the desired size.\n        # This is a flexible way to handle mismatches between backbone output and desired grid size.\n        x = backbone_output\n        print(f\"Resized feature map shape: {x.shape}\")\n    \n        x = Conv2D(512, (1, 1), padding='same')(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = Conv2D(1024, (3, 3), padding='same')(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        x = Conv2D(512, (1, 1), padding='same')(x)\n        x = LeakyReLU(alpha=0.1)(x)\n        \n        num_filters = NUM_ANCHORS * (5 + NUM_CLASSES)\n        predictions_raw = Conv2D(num_filters, (1, 1), padding='same', name='detection_head')(x)\n        \n        model = Model(inputs=input_tensor, outputs=predictions_raw)\n        return model\n# 1. Build the model architecture\nprint(\"Building model architecture...\")\nmodel = build_model_640()\n\n# 2. Load the trained weights\nprint(\"Loading trained weights from 'best_model_enhanced_1.weights.h5'...\")\nmodel.load_weights('best_model_enhanced_1.weights.h5')\nprint(\"Weights loaded successfully.\")\n\n\n# --- Helper Functions for Evaluation ---\n\ndef decode_predictions(raw_preds, confidence_thresh=0.7): # Lower the initial threshold here\n    \"\"\"\n    Decodes the raw output of the YOLO model and flattens the output\n    to be compatible with tf.image.non_max_suppression.\n    \"\"\"\n    grid_h, grid_w = raw_preds.shape[1:3]\n    \n    # Reshape to (batch_size, grid_h, grid_w, num_anchors, 5 + num_classes)\n    predictions = tf.reshape(raw_preds, (tf.shape(raw_preds)[0], grid_h, grid_w, NUM_ANCHORS, 5 + NUM_CLASSES))\n\n    # Get individual components\n    pred_xy = tf.sigmoid(predictions[..., 0:2])\n    pred_wh_offset = predictions[..., 2:4]\n    pred_conf = tf.sigmoid(predictions[..., 4:5])\n    pred_class_probs = tf.nn.softmax(predictions[..., 5:])\n\n    # Create a grid for calculating absolute coordinates\n    grid = tf.meshgrid(tf.range(grid_w, dtype=tf.float32), tf.range(grid_h, dtype=tf.float32))\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n\n    # Decode box coordinates\n    box_xy_grid = pred_xy + grid\n    box_xy_normalized = box_xy_grid / tf.constant([grid_w, grid_h], dtype=tf.float32)\n    \n    anchors_tensor = tf.constant(ANCHORS, dtype=tf.float32)\n    box_wh_pixels = tf.exp(pred_wh_offset) * anchors_tensor\n    box_wh_normalized = box_wh_pixels / tf.constant([IMAGE_SIZE[1], IMAGE_SIZE[0]], dtype=tf.float32)\n\n    box_x1y1 = box_xy_normalized - (box_wh_normalized / 2.0)\n    box_x2y2 = box_xy_normalized + (box_wh_normalized / 2.0)\n    boxes = tf.concat([box_x1y1, box_x2y2], axis=-1)\n\n    # Calculate final scores and get class predictions\n    final_scores = pred_conf * pred_class_probs\n    class_ids = tf.argmax(final_scores, axis=-1)\n    max_scores = tf.reduce_max(final_scores, axis=-1)\n\n    # --- THE FIX IS HERE ---\n    # We flatten the grid and anchor dimensions to get a list of boxes per image.\n    # The new shape will be (batch_size, num_all_boxes, 4)\n    boxes_flat = tf.reshape(boxes, (tf.shape(boxes)[0], -1, 4))\n    \n    # Do the same for scores and classes\n    # New shape: (batch_size, num_all_boxes)\n    scores_flat = tf.reshape(max_scores, (tf.shape(max_scores)[0], -1))\n    classes_flat = tf.reshape(class_ids, (tf.shape(class_ids)[0], -1))\n\n    # Optional: Filter out boxes with very low scores early to speed up NMS\n    # This is not strictly necessary but is good practice.\n    score_mask = scores_flat > confidence_thresh\n    \n    final_boxes = tf.boolean_mask(boxes_flat, score_mask)\n    final_scores = tf.boolean_mask(scores_flat, score_mask)\n    final_classes = tf.boolean_mask(classes_flat, score_mask)\n    \n    # NMS expects a single tensor for each, not batched, so we remove the batch dim\n    # This assumes we are always processing one image at a time in evaluation\n    # To handle batches, you would loop or use tf.map_fn\n    \n    # We will return the flattened tensors and let the main loop handle NMS\n    return boxes_flat, scores_flat, classes_flat\n\ndef calculate_iou(box1, box2):\n    \"\"\"Calculates IoU for two sets of boxes.\n    box format: [x1, y1, x2, y2]\n    \"\"\"\n    x1_inter = np.maximum(box1[:, 0], box2[:, 0])\n    y1_inter = np.maximum(box1[:, 1], box2[:, 1])\n    x2_inter = np.minimum(box1[:, 2], box2[:, 2])\n    y2_inter = np.minimum(box1[:, 3], box2[:, 3])\n\n    inter_area = np.maximum(0.0, x2_inter - x1_inter) * np.maximum(0.0, y2_inter - y1_inter)\n\n    box1_area = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n    box2_area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n    \n    union_area = box1_area + box2_area - inter_area\n    \n    return inter_area / (union_area + 1e-6)\n\n# --- Main Evaluation Logic ---\n\ndef evaluate_model(model, img_dir, lbl_dir, conf_thresh=0.5, nms_thresh=0.5, iou_threshold=0.5):\n    \"\"\"\n    Main function to calculate mAP and other metrics.\n    \"\"\"\n    all_detections = []\n    all_ground_truths = []\n    \n    validation_files = [os.path.splitext(f)[0] for f in os.listdir(img_dir) if f.endswith('.jpg')]\n\n    print(f\"\\nRunning evaluation on {len(validation_files)} validation images...\")\n    \n    for i, filename in enumerate(tqdm(validation_files)):\n        # --- 1. Get Predictions ---\n        image_path = os.path.join(img_dir, filename + '.jpg')\n        image = cv2.imread(image_path)\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n        input_data = np.expand_dims(image_resized / 255.0, axis=0)\n        \n        raw_preds = model.predict(input_data, verbose=0)\n        \n        # Decode predictions and apply NMS\n        pred_boxes, pred_scores, pred_classes = decode_predictions(raw_preds, confidence_thresh)\n        \n        # Non-Max Suppression\n        selected_indices = tf.image.non_max_suppression(\n            boxes=pred_boxes[0],\n            scores=pred_scores[0],\n            max_output_size=50,\n            iou_threshold=nms_thresh\n        )\n        \n        final_boxes = tf.gather(pred_boxes[0], selected_indices).numpy()\n        final_scores = tf.gather(pred_scores[0], selected_indices).numpy()\n        final_classes = tf.gather(pred_classes[0], selected_indices).numpy()\n        \n        for box, score, cls in zip(final_boxes, final_scores, final_classes):\n            # Store as [image_idx, class_id, confidence, x1, y1, x2, y2]\n            all_detections.append([i, int(cls), float(score), *box])\n            \n        # --- 2. Get Ground Truths ---\n        label_path = os.path.join(lbl_dir, filename + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    class_id = int(parts[0])\n                    x_c, y_c, w, h = map(float, parts[1:])\n                    x1 = x_c - w / 2\n                    y1 = y_c - h / 2\n                    x2 = x_c + w / 2\n                    y2 = y_c + h / 2\n                    # Store as [image_idx, class_id, 0 (not a detection), x1, y1, x2, y2]\n                    all_ground_truths.append([i, class_id, 0, x1, y1, x2, y2])\n                    \n    # --- 3. Calculate mAP ---\n    average_precisions = {}\n    \n    # Process one class at a time (here we only have one class: 0)\n    for class_id in range(NUM_CLASSES):\n        # Get all detections and ground truths for this class\n        detections_c = [d for d in all_detections if d[1] == class_id]\n        ground_truths_c = [gt for gt in all_ground_truths if gt[1] == class_id]\n\n        if len(ground_truths_c) == 0:\n            average_precisions[class_id] = 1.0 if len(detections_c) == 0 else 0.0\n            continue\n            \n        # Sort detections by confidence score (high to low)\n        detections_c.sort(key=lambda x: x[2], reverse=True)\n        \n        num_gt_boxes = len(ground_truths_c)\n        true_positives = np.zeros(len(detections_c))\n        false_positives = np.zeros(len(detections_c))\n        \n        # Keep track of which ground truth boxes have been matched\n        gt_matched = {} # key: image_idx, value: list of matched gt box indices\n\n        for det_idx, detection in enumerate(detections_c):\n            img_idx = detection[0]\n            \n            # Get ground truths for the same image\n            gts_in_image = [gt for gt in ground_truths_c if gt[0] == img_idx]\n            \n            if len(gts_in_image) == 0:\n                false_positives[det_idx] = 1\n                continue\n            \n            det_box = np.array([detection[3:]])\n            gt_boxes = np.array([gt[3:] for gt in gts_in_image])\n            \n            ious = calculate_iou(det_box, gt_boxes)\n            best_iou_idx = np.argmax(ious)\n            \n            if ious[best_iou_idx] >= iou_threshold:\n                # Check if this ground truth box was already matched\n                if gt_matched.get(img_idx) is None:\n                    gt_matched[img_idx] = []\n                \n                if best_iou_idx not in gt_matched[img_idx]:\n                    true_positives[det_idx] = 1\n                    gt_matched[img_idx].append(best_iou_idx)\n                else: # Matched a GT box that was already used\n                    false_positives[det_idx] = 1\n            else: # Did not meet IoU threshold\n                false_positives[det_idx] = 1\n\n        # Calculate precision and recall\n        tp_cumsum = np.cumsum(true_positives)\n        fp_cumsum = np.cumsum(false_positives)\n        \n        recalls = tp_cumsum / (num_gt_boxes + 1e-6)\n        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n        \n        # AP is the area under the precision-recall curve\n        precisions = np.concatenate(([1.0], precisions))\n        recalls = np.concatenate(([0.0], recalls))\n        \n        # Replace each precision value with the maximum precision value to its right\n        for i in range(len(precisions) - 2, -1, -1):\n            precisions[i] = np.maximum(precisions[i], precisions[i+1])\n            \n        # Calculate the area\n        recall_diff = recalls[1:] - recalls[:-1]\n        ap = np.sum(recall_diff * precisions[1:])\n        average_precisions[class_id] = ap\n\n    mAP = np.mean(list(average_precisions.values()))\n    \n    print(\"\\n--- Evaluation Metrics ---\")\n    print(f\"Confidence Threshold: {conf_thresh}\")\n    print(f\"NMS Threshold: {nms_thresh}\")\n    print(f\"IoU Threshold for mAP: {iou_threshold}\")\n    print(\"--------------------------\")\n    for class_id, ap in average_precisions.items():\n        print(f\"AP for class {class_id} (Vehicle): {ap:.4f}\")\n    print(f\"\\nMean Average Precision (mAP)@{iou_threshold}: {mAP:.4f}\")\n    \n    return mAP, average_precisions\n\ndef visualize_evaluation(model, img_dir, lbl_dir, num_images=5, conf_thresh=0.25, nms_thresh=0.5):\n    \"\"\"\n    Draws ground truth boxes and prediction boxes on a few sample images.\n    INCLUDES THE CRUCIAL SCORE THRESHOLD.\n    \"\"\"\n    validation_files = [os.path.splitext(f)[0] for f in os.listdir(img_dir) if f.endswith('.jpg')]\n    if len(validation_files) < num_images:\n        print(f\"Warning: Requesting {num_images} images to visualize, but only {len(validation_files)} are available.\")\n        num_images = len(validation_files)\n        \n    sample_files = np.random.choice(validation_files, num_images, replace=False)\n\n    for filename in sample_files:\n        image_path = os.path.join(img_dir, filename + '.jpg')\n        image = cv2.imread(image_path)\n        original_h, original_w, _ = image.shape\n        \n        # Prepare image for model\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n        input_data = np.expand_dims(image_resized / 255.0, axis=0)\n\n        # Get Predictions using the corrected decode function\n        pred_boxes, pred_scores, pred_classes = decode_predictions(model.predict(input_data, verbose=0))\n\n        # --- THE FIX IS HERE ---\n        # Add the `score_threshold` parameter to the NMS call.\n        # This will first discard all boxes with a score < conf_thresh, THEN run NMS.\n        selected_indices = tf.image.non_max_suppression(\n            boxes=pred_boxes[0],\n            scores=pred_scores[0],\n            max_output_size=50,\n            iou_threshold=nms_thresh,\n            score_threshold=conf_thresh  # <-- THIS IS THE CRUCIAL FIX\n        )\n        \n        final_boxes = tf.gather(pred_boxes[0], selected_indices).numpy()\n        final_scores = tf.gather(pred_scores[0], selected_indices).numpy()\n\n        # Draw Prediction Boxes (in green)\n        for box, score in zip(final_boxes, final_scores):\n            x1, y1, x2, y2 = box\n            x1 = int(x1 * original_w)\n            y1 = int(y1 * original_h)\n            x2 = int(x2 * original_w)\n            y2 = int(y2 * original_h)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            label = f\"Pred: {score:.2f}\"\n            cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Draw Ground Truth Boxes (in blue)\n        label_path = os.path.join(lbl_dir, filename + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    _, x_c, y_c, w, h = map(float, line.strip().split())\n                    x1 = int((x_c - w/2) * original_w)\n                    y1 = int((y_c - h/2) * original_h)\n                    x2 = int((x_c + w/2) * original_w)\n                    y2 = int((y_c + h/2) * original_h)\n                    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n                    cv2.putText(image, 'Ground Truth', (x1, y2+15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n        \n        plt.figure(figsize=(12, 12))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.title(f\"Evaluation: {filename}.jpg\")\n        plt.axis('off')\n        plt.show()\n# --- Run the Evaluation ---\nif __name__ == '__main__':\n    # Calculate mAP@0.5 (the standard PASCAL VOC metric)\n    evaluate_model(model, VALID_IMG_DIR, VALID_LBL_DIR, iou_threshold=0.2)\n    \n    # You could also calculate mAP for a stricter IoU\n    # evaluate_model(model, VALID_IMG_DIR, VALID_LBL_DIR, iou_threshold=0.75)\n    \n    # Visualize some results\n    print(\"\\nDisplaying sample evaluation images (Green=Prediction, Blue=Ground Truth)...\")\n    visualize_evaluation(model, VALID_IMG_DIR, VALID_LBL_DIR, num_images=5)\n\"\"\"     \"\"\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Testing on best_model_enhanced_1.weights.h5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nprint(\"\\n--- Loading best weights for inference ---\")\nmodel.load_weights('best_model_enhanced_1.weights.h5')\n\ndef decode_and_nms(predictions, confidence_thresh=0.5, nms_thresh=0.2):\n    grid_h, grid_w = predictions.shape[1:3]\n    predictions = tf.reshape(predictions, (tf.shape(predictions)[0], grid_h, grid_w, NUM_ANCHORS, 5 + NUM_CLASSES))\n    pred_xy = tf.sigmoid(predictions[..., 0:2])\n    pred_wh_offset = predictions[..., 2:4]\n    pred_conf = tf.sigmoid(predictions[..., 4:5])\n    pred_class = tf.nn.softmax(predictions[..., 5:])\n    grid = tf.meshgrid(tf.range(grid_w), tf.range(grid_h))\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n    grid = tf.cast(grid, tf.float32)\n    box_center_grid = pred_xy + grid\n    box_wh_pixels = tf.exp(pred_wh_offset) * ANCHORS\n    box_xy_normalized = box_center_grid / tf.constant([grid_w, grid_h], dtype=tf.float32)\n    box_wh_normalized = box_wh_pixels / tf.constant([IMAGE_SIZE[0], IMAGE_SIZE[1]], dtype=tf.float32)\n    box_x1y1 = box_xy_normalized - (box_wh_normalized / 2)\n    box_x2y2 = box_xy_normalized + (box_wh_normalized / 2)\n    boxes_tensor = tf.concat([box_x1y1, box_x2y2], axis=-1)\n    final_scores = pred_conf * pred_class\n    max_scores = tf.reduce_max(final_scores, axis=-1)\n    mask = max_scores >= confidence_thresh\n    boxes_filtered = tf.boolean_mask(boxes_tensor, mask)\n    scores_filtered = tf.boolean_mask(max_scores, mask)\n    selected_indices = tf.image.non_max_suppression(\n        boxes_filtered, scores_filtered, max_output_size=50, iou_threshold=nms_thresh\n    )\n    final_boxes = tf.gather(boxes_filtered, selected_indices)\n    final_scores = tf.gather(scores_filtered, selected_indices)\n    return final_boxes.numpy(), final_scores.numpy()    \n\ndef run_inference_and_draw(model, image_path, confidence_thresh=0.6, nms_thresh=0.2):\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Error: Could not read image at {image_path}\")\n        return None\n    original_h, original_w, _ = image.shape\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_resized = cv2.resize(image_rgb, IMAGE_SIZE)\n    input_data = image_resized / 255.0\n    input_data = np.expand_dims(input_data, axis=0)\n    raw_preds = model.predict(input_data, verbose=0)\n    boxes, scores = decode_and_nms(raw_preds, confidence_thresh=confidence_thresh, nms_thresh=nms_thresh)\n    print(\"\\n--- Decoded Boxes (normalized coordinates) ---\")\n    print(boxes)\n    print(\"--------------------------------------------\")\n    print(f\"Found {len(boxes)} vehicles in the image.\")\n    for i, box in enumerate(boxes):\n        x1, y1, x2, y2 = box\n        x1, y1 = max(0, x1), max(0, y1)\n        x2, y2 = min(1, x2), min(1, y2)\n        x1, y1 = int(x1 * original_w), int(y1 * original_h)\n        x2, y2 = int(x2 * original_w), int(y2 * original_h)\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        label = f\"Vehicle: {scores[i]:.2f}\"\n        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 2)\n    return image\nfor i in range(90):\n    \n    try:\n        validation_image_files = os.listdir(VALID_IMG_DIR)\n        if validation_image_files:\n            sample_image_name = validation_image_files[i]\n            sample_image_path = os.path.join(VALID_IMG_DIR, sample_image_name)\n            print(f\"\\n--- Running visualization on: {sample_image_path} ---\")\n            result_image = run_inference_and_draw(model, sample_image_path)\n            if result_image is not None:\n                plt.figure(figsize=(12, 12))\n                plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n                plt.axis('off')\n                plt.title(\"Single Image Inference Result\")\n                plt.show()\n        else:\n            print(\"No images found in validation directory to run visualization.\")\n    except Exception as e:\n        print(f\"Could not run visualization. Error: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}